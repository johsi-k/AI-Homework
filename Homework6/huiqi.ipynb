{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Jul  3 04:44:44 2018\n",
    "\n",
    "@author: ho\n",
    "\"\"\"\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Jun 25 11:08:38 2018\n",
    "\n",
    "@author: ho\n",
    "\"\"\"\n",
    "\n",
    "## 50.021 Artificial Intelligence HW6\n",
    "## Done By: Ho Hui Qi 1001612\n",
    "## Run code using python v3.6.3, coded in Spyder IDE, Windows\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import string\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD DATA INTO DICT\n",
    "# =============================================================================\n",
    "def getInput(filepath):  \n",
    "    '''Takes in a filepath to the csv file with lines from \"star trek\"\n",
    "    and splits into lines, then returns a dictionary with 'star trek' as key\n",
    "    and a list of lines as value'''\n",
    "    # create an empty dictionary\n",
    "    category_lines = {}\n",
    "    # create an empty list to store the lines\n",
    "    lines = []\n",
    "    # define 'filterowrds'\n",
    "    filterwords = ['NEXTEPISODE']\n",
    "    # open csv file with star trek lines\n",
    "    with open(filepath,'r') as f:\n",
    "        for line in f:\n",
    "#            print (line)\n",
    "            # remove the spaces and replace symbols with spaces\n",
    "            v1 = line.strip().replace('=','').replace('/',' ').replace('+',' ').replace('(',' ')\n",
    "            v2 = v1.replace('[',' ').replace(')',' ').replace(']',' ').split(',')\n",
    "#            print (v2)\n",
    "            # for each line (list)\n",
    "            for w in v2:\n",
    "                # check that it is not filterwords and is longer than 1\n",
    "                # meaning it is legit line\n",
    "                if (w not in filterwords) and (len(w)>1):\n",
    "                    if w[0] == ' ':\n",
    "                        w = w[1:]\n",
    "                    # append the line to our list \n",
    "                    lines.append(w)\n",
    "    # add the list to our dictionary with 'star trek' as key\n",
    "    category_lines['star trek'] = lines\n",
    "    # return the dictionary\n",
    "    return category_lines\n",
    "\n",
    "# =============================================================================\n",
    "# CREATE THE LSTM NETWORK\n",
    "# =============================================================================\n",
    "class RNNLSTM(nn.Module):\n",
    "    # add dropout probability\n",
    "    def __init__(self, inp_size, hidden_dim, num_letters, n_layers, dropout_prob):\n",
    "        super(RNNLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.num_letters = num_letters\n",
    "        self.lstm = nn.LSTM(input_size=inp_size, hidden_size=hidden_dim, num_layers=n_layers, dropout=dropout_prob)\n",
    "        self.fc = nn.Linear(in_features=hidden_dim, out_features=num_letters)\n",
    "        self.ls = torch.nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    # no need batchsize\n",
    "    def forward(self, x0, h0, c0, debug = False):\n",
    "        # x0 is a list of tensors \n",
    "        x = torch.nn.utils.rnn.pack_sequence(x0) \n",
    "        # get the output, hidden state and memory of the nlayer lstm\n",
    "        output, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        # get the hidden state from the second LSTM layer\n",
    "        preout = hn[self.n_layers-1,:,:]\n",
    "        # run preout through fully connected layer and get soft max\n",
    "        # for NLL loss, use log softmax\n",
    "        z = self.ls(self.fc(preout))\n",
    "        return z\n",
    "        \n",
    "# =============================================================================\n",
    "# CONVERT TO TENSORS\n",
    "# =============================================================================\n",
    "'''\n",
    "# One-hot vector for category\n",
    "def categoryTensor(category):\n",
    "    li = all_categories.index(category)\n",
    "    tensor = torch.zeros(1, n_categories)\n",
    "    tensor[0][li] = 1\n",
    "    return tensor\n",
    "'''\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'-:?!\" + \"0123456789\" \n",
    "#print (all_letters[52])\n",
    "n_letters = len(all_letters) + 1 # Plus EOS marker\n",
    "\n",
    "# One-hot matrix of first to last letters (not including EOS) for input\n",
    "def inputTensor(line):\n",
    "    '''Takes in a string (line of letters) and converts into tensor\n",
    "    with dimensions [length of line, 1, total number of letters]; \n",
    "    inputs into our LSTM network'''\n",
    "    tensor = torch.zeros(len(line),1,n_letters)\n",
    "    for letter_idx in range(len(line)):\n",
    "        letter = line[letter_idx]\n",
    "        tensor[letter_idx][0][all_letters.find(letter)] = 1\n",
    "    # add the EOS token\n",
    "#    tensor[len(line)][0][n_letters-1] = 1\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "# LongTensor of second letter to end (EOS) for target\n",
    "def targetTensor(line):\n",
    "    '''Takes in a string (line of letters) and converts into tensor\n",
    "    with dimensions [length of line]; targets or true labels'''\n",
    "    letter_indexes = [all_letters.find(letter) for letter in line[1:]]\n",
    "#    print (letter_indexes)\n",
    "    letter_indexes.append(n_letters-1) # EOS\n",
    "#    print(len(letter_indexes))\n",
    "    return torch.LongTensor(letter_indexes)\n",
    "\n",
    "# =============================================================================\n",
    "# SPLIT DATA INTO TRAINING AND TESTING\n",
    "# =============================================================================\n",
    "# Use 80% for training and 20% for testing\n",
    "# Temperatured sampling?\n",
    "    \n",
    "# split data based on trainratio\n",
    "def dataSpliter(data, trainratio):\n",
    "    '''Takes in a list of lines from dictionary 'data' (with key 'st')\n",
    "    and splits the list based on 'trainratio', returns 2 lists, one with\n",
    "    the lines for training and another for testing'''\n",
    "    trainlist = []\n",
    "    testlist = []\n",
    "    # list of lines from data (dictionary) \n",
    "    linelist = data['star trek']\n",
    "    # shuffle the list\n",
    "    random.shuffle(linelist)\n",
    "    cut = int(trainratio*len(linelist))\n",
    "    trainlist = linelist[0:cut]\n",
    "    testlist = linelist[cut:]\n",
    "    return trainlist, testlist\n",
    "\n",
    "#print (trainlist)\n",
    "#print (testlist)\n",
    "'''\n",
    "traintensors = []\n",
    "for line in trainlist:\n",
    "    t = inputTensor(line)\n",
    "    traintensors.append(t)\n",
    "\n",
    "testtensors = []\n",
    "for line in testlist:\n",
    "    t = inputTensor(line)\n",
    "    testtensors.append(t)\n",
    "\n",
    "print (traintensors)\n",
    "print (testtensors)\n",
    "'''\n",
    "\n",
    "def sampleletter():\n",
    "    letters = string.ascii_uppercase\n",
    "    letters = letters.replace('Q','').replace('X','').replace('Y','')\n",
    "    idx = random.randint(0,len(letters)-1)\n",
    "#    print (letters)\n",
    "    return letters[idx]\n",
    "\n",
    "def main(hdim, nlayer, epochs):\n",
    "    # Training settings\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # extract data from star trek file and store in datadict with 'st' as key\n",
    "    datadict = getInput('./star_trek_transcripts_all_episodes_f.csv')\n",
    "    \n",
    "    # split 'st' data set into training and testing dataset \n",
    "    trainlist, testlist = dataSpliter(datadict, trainratio=0.8)\n",
    "    \n",
    "    # initialize lstm model\n",
    "    lstm = RNNLSTM(inp_size=n_letters, hidden_dim=hdim, num_letters=n_letters, \n",
    "                   n_layers=nlayer, dropout_prob=0.1).to(device)\n",
    "    \n",
    "    # define the loss function for backpropagation\n",
    "    loss_function = nn.NLLLoss()\n",
    "    \n",
    "    print ('Predicting Star Trek lines \\n')\n",
    "    print ('LSTM with', hdim, 'hidden dimensions &', nlayer, 'layers')\n",
    "    print ('------------------------------------')\n",
    "    \n",
    "    # set learning rate\n",
    "    learning_rate = 0.005\n",
    "    \n",
    "    # intialise empty lists to store losses\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    test_acc = []\n",
    "    \n",
    "    # define a best acc value to compare against\n",
    "    bestacc = 0\n",
    "    \n",
    "    for ep in range(epochs):   \n",
    "        print ('epoch', ep, 'of', epochs, ':')\n",
    "        \n",
    "        # =====================================================================\n",
    "        # TRAINING PHASE\n",
    "        # =====================================================================\n",
    "        print (' TRAINING...')\n",
    "        avgtrainloss = 0\n",
    "        \n",
    "        for line in trainlist:\n",
    "#            print (line)\n",
    "            # convert input line to tensor of characters\n",
    "            inp = inputTensor(line).to(device)\n",
    "            # convert input line to tensor of indexes\n",
    "            targ = targetTensor(line).unsqueeze(-1).to(device)\n",
    "            \n",
    "            # initlialize hidden laeyrs and cell state\n",
    "            h0 = torch.zeros(nlayer,1,hdim)\n",
    "            c0 = torch.zeros(nlayer,1,hdim)\n",
    "            \n",
    "            # set lstm to train mode\n",
    "            lstm.train()\n",
    "            \n",
    "            # set lstm to zero\n",
    "            lstm.zero_grad()\n",
    "            \n",
    "            # initialise losses\n",
    "            losses = 0\n",
    "            avgloss = 0\n",
    "        \n",
    "            for i in range(inp.shape[0]):\n",
    "                # pass input into the lstm\n",
    "                output = lstm(inp[i].unsqueeze(0), h0, c0)\n",
    "                # keep a copy of the output to prevent aliasing\n",
    "                outputcopy = output.clone()\n",
    "                # calculate the loss with NLL loss\n",
    "                ls = loss_function(outputcopy,targ[i])\n",
    "                # add the losses for each line\n",
    "                losses += ls\n",
    "                # total losses per line\n",
    "                avgloss += ls.item()\n",
    "            # average loss for each line\n",
    "            avgtrainloss += avgloss/len(line)\n",
    "            # backpropagate over losses\n",
    "            losses.backward()\n",
    "\n",
    "            # update parameters            \n",
    "            for p in lstm.parameters():\n",
    "                p.data.add_(-learning_rate, p.grad.data)\n",
    "        trainloss = avgtrainloss/len(trainlist)\n",
    "        print('Train avgloss', trainloss)\n",
    "        # add to the train_loss list for each epoch\n",
    "        train_loss.append(trainloss)\n",
    "\n",
    "        # =====================================================================\n",
    "        # SAMPLING PHASE (TEMPERATURED)\n",
    "        # =====================================================================\n",
    "        print ('\\n SAMPLING...')\n",
    "        # define a max length of sequence to output\n",
    "        max_length = 20\n",
    "        # define the number of samples we want\n",
    "        n_samples = 10\n",
    "        # define temperature\n",
    "        temp = 0.5\n",
    "        \n",
    "        for s in range(n_samples):\n",
    "            # randomly pick a starting letter\n",
    "            start_letter = sampleletter()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # convert start letter to input tensor\n",
    "                test = inputTensor(start_letter)\n",
    "                # add start letter to output line\n",
    "                outputline = start_letter\n",
    "                \n",
    "                # initialise hidden layers and cell state\n",
    "                h0 = torch.zeros(nlayer,1,hdim)\n",
    "                c0 = torch.zeros(nlayer,1,hdim)\n",
    "                \n",
    "                # set lstm to eval mode\n",
    "                lstm.eval()\n",
    "                \n",
    "                # for each character in the sample line\n",
    "                for i in range(max_length):\n",
    "                    # pass input through lstm\n",
    "                    output = lstm(test, h0, c0)\n",
    "                    # get temperatured output, softmax it\n",
    "                    tempout = torch.nn.functional.softmax(output/temp, dim=1)\n",
    "                    # get a sample from the multinomial dist\n",
    "                    samplei = torch.multinomial(tempout,1,replacement=False)\n",
    "                    # find the index of the sampled letter\n",
    "                    samplei = samplei[0][0]\n",
    "                    \n",
    "                    if samplei == n_letters - 1: #EOS\n",
    "                        break\n",
    "                    else:\n",
    "                        # retrieve the best letter\n",
    "                        letter = all_letters[samplei]\n",
    "                        # append letter to list \n",
    "                        outputline += letter\n",
    "                    # use the best letter as the input \n",
    "                    test = inputTensor(letter)\n",
    "                    \n",
    "                print (outputline)\n",
    "            \n",
    "        # =====================================================================\n",
    "        # TESTING PHASE\n",
    "        # =====================================================================\n",
    "        print ('\\n TESTING...')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            avgtestloss = 0\n",
    "            avgtestacc = 0\n",
    "            \n",
    "            # for each line in the test set\n",
    "            for line in testlist:\n",
    "                '''\n",
    "                outputline = line[0]\n",
    "                '''\n",
    "                # convert line to input tensors\n",
    "                test = inputTensor(line).to(device)\n",
    "                # convert line to target tensor\n",
    "                targ = targetTensor(line).unsqueeze(-1).to(device)\n",
    "\n",
    "                # initialise hidden layers and cell state\n",
    "                h0 = torch.zeros(nlayer,1,hdim)\n",
    "                c0 = torch.zeros(nlayer,1,hdim)\n",
    "                \n",
    "                # initialise losses\n",
    "                losses = 0\n",
    "                testacc = 0\n",
    "                \n",
    "                # set lstm to eval mode\n",
    "                lstm.eval()\n",
    "                \n",
    "                # for each character in the line\n",
    "                for i in range(len(line)):\n",
    "                    # pass character through the lstm\n",
    "                    output = lstm(test[i].unsqueeze(0), h0, c0)\n",
    "                    # copy the output to prevent aliasing\n",
    "                    outputcopy = output.clone()\n",
    "                    # find the loss using loss function\n",
    "                    ls = loss_function(outputcopy,targ[i])\n",
    "                    # find the best letter prediction\n",
    "                    topv, topi = outputcopy.topk(1)\n",
    "                    # get the index of the best prediction\n",
    "                    topi = topi[0][0]\n",
    "                    # check if the predcition matches the actual letter\n",
    "                    testacc += (topi==targ[i]).item()\n",
    "                    # sum losses over each line\n",
    "                    losses += ls.item()\n",
    "                    \n",
    "                    if topi == n_letters-1: #EOS\n",
    "                        break\n",
    "                    else:\n",
    "                        # find the letter corresponding to the best index\n",
    "                        letter = all_letters[topi]\n",
    "                        '''\n",
    "                        outputline += letter\n",
    "                        '''\n",
    "                '''\n",
    "                print (line, '//', outputline)\n",
    "                '''\n",
    "                # average accuracy over each line\n",
    "                avgtestacc += testacc/len(outputline)\n",
    "                # average losses over each line\n",
    "                avgtestloss += losses/len(outputline)\n",
    "                \n",
    "        testloss = avgtestloss/len(testlist)\n",
    "        testacc = avgtestacc/len(testlist)\n",
    "        \n",
    "        # add average test loss per eopch to list\n",
    "        test_loss.append(testloss)\n",
    "        print('Test avgloss', testloss)\n",
    "        # add average test accuracy per eopch to list\n",
    "        test_acc.append(testacc*100)\n",
    "        print('Test accuracy', testacc*100, '%')\n",
    "        print ('\\n')\n",
    "        \n",
    "        # keep track of the weights that give the best test accuracy\n",
    "        if testacc > bestacc:\n",
    "            bestacc = testacc\n",
    "            best_model_wts = copy.deepcopy(lstm.state_dict())\n",
    "    \n",
    "    # save this model in a file named 'model.pt'\n",
    "    torch.save(best_model_wts,'./model.pt')     \n",
    "    \n",
    "    # =========================================================================\n",
    "    # PLOTS\n",
    "    # =========================================================================\n",
    "    x = list(range(epochs))\n",
    "    \n",
    "    ax = plt.subplot(311)\n",
    "    # plot train and test error on the same graph\n",
    "    ax.plot(x, train_loss, 'r', label='train error')\n",
    "    ax.plot(x, test_loss, 'b', label='test error')\n",
    "    ax.set_title('Errors with epochs')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Errors')        \n",
    "    \n",
    "    ax.legend()\n",
    "    \n",
    "    bx = plt.subplot(313)\n",
    "    # plot test accuracy on a separate graph\n",
    "    bx.plot(x, test_acc, 'b', label='test accuracy')\n",
    "    bx.set_title('Test accuracy with epochs')\n",
    "    bx.set_xlabel('Epochs')\n",
    "    bx.set_ylabel('Test Accuracy') \n",
    "\n",
    "    # show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "main(hdim=200, nlayer=2, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
