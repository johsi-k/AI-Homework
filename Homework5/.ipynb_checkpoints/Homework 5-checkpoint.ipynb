{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-27T17:15:55.968877Z",
     "start_time": "2018-06-27T17:15:55.958882Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-27T17:15:57.560964Z",
     "start_time": "2018-06-27T17:15:57.380066Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import copy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def findFiles(path): return glob.glob(path)\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "# Build the category_lines dictionary, a list of names per language\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "# Partition data\n",
    "def partition(data, ratio=0.3):\n",
    "    shuffled = random.sample(data, k=len(data))\n",
    "    split_idx = int(len(shuffled)*0.3)\n",
    "    return shuffled[:split_idx], shuffled[split_idx:]\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    filename_norm = os.path.normpath(filename)\n",
    "#     print(filename_norm)\n",
    "    category = filename_norm.split('\\\\')[-1].split('.')[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename_norm)\n",
    "#     print(lines)\n",
    "    category_lines[category] = partition(lines)\n",
    "\n",
    "n_categories = len(all_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-27T17:15:58.886205Z",
     "start_time": "2018-06-27T17:15:58.877210Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# print(letterToTensor('J'))\n",
    "# print(lineToTensor('Jones').shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-27T17:16:00.278408Z",
     "start_time": "2018-06-27T17:16:00.267414Z"
    }
   },
   "outputs": [],
   "source": [
    "# category-index mapping\n",
    "category_index = {c:i for i, c in enumerate(all_categories)}\n",
    "    \n",
    "class WordDataset(Dataset):\n",
    "    def __init__(self, wordnamelist, train_test_idx):\n",
    "        self.word_cat_list = []\n",
    "        \n",
    "        for cat, words in category_lines.items():\n",
    "            for word in words[train_test_idx]:\n",
    "                name = lineToTensor(word)\n",
    "                category = torch.tensor([category_index[cat]])\n",
    "                self.word_cat_list.append((name, category))\n",
    "                \n",
    "        random.shuffle(self.word_cat_list)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        word, cat = self.word_cat_list[index]\n",
    "#         print('category', cat.shape, '\\n' 'word', word.shape)\n",
    "        return word, cat\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.word_cat_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-27T17:16:01.573669Z",
     "start_time": "2018-06-27T17:16:01.566671Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    # batch = [(tensor, label), (tensor, label),...]\n",
    "    # sort batch in descending order of tensor sequence length\n",
    "    sorted_batch = sorted(batch, key=lambda x:x[0].shape[0], reverse=True)\n",
    "    \n",
    "    # sends each (tensor, label) in sorted batch into zip\n",
    "    tensor, label = zip(*sorted_batch)\n",
    "    \n",
    "#     print('label', label[0].shape)\n",
    "    return torch.nn.utils.rnn.pack_sequence(tensor), torch.cat(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-27T17:16:04.162850Z",
     "start_time": "2018-06-27T17:16:04.154854Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMmodel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "        self.lstm = torch.nn.LSTM(input_size=input_size,\n",
    "                     hidden_size=hidden_size,\n",
    "                     num_layers = num_layers\n",
    "                    )\n",
    "        self.fc = torch.nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         print('x','\\n',x)\n",
    "        # x is PackedSequence\n",
    "        h_0 = torch.zeros(self.num_layers, x.batch_sizes[0], self.hidden_size)\n",
    "        c_0 = torch.zeros(self.num_layers, x.batch_sizes[0], self.hidden_size)\n",
    "        output, (h_n, c_n) = self.lstm(x, (h_0, c_0))\n",
    "        output = self.fc(h_n[-1,:,:])\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-27T17:16:06.507000Z",
     "start_time": "2018-06-27T17:16:06.498990Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, test_dataloader, train_data_len):\n",
    "    \n",
    "    cross_entropy = nn.CrossEntropyLoss(size_average=False)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "    \n",
    "    best_model_weights = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    total_iter = train_data_len / train_dataloader.batch_size\n",
    "    \n",
    "    for epoch in range(1,6):\n",
    "        running_loss = 0\n",
    "        totals = 0\n",
    "        iteration = 0\n",
    "        model.train()\n",
    "        \n",
    "        for packedseq, label in train_dataloader:\n",
    "            iteration += 1\n",
    "            optimizer.zero_grad()\n",
    "            output = model(packedseq)\n",
    "            \n",
    "            loss = cross_entropy(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            totals += label.shape[0]\n",
    "            \n",
    "            print('iter: {}/{}, running loss = {:.4f}'.format(iteration, total_iter, running_loss), end='\\r')\n",
    "            \n",
    "        training_loss = running_loss / totals\n",
    "        test_loss, test_acc = test(model, test_dataloader)\n",
    "            \n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_model_weights = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "        print('Epoch: {} \\tTraining Loss: {:.4f} \\tTest Loss: {:.4f} \\tTest Accuracy: {:.4f}'.format(\n",
    "               epoch, training_loss, test_loss, test_acc))\n",
    "        \n",
    "    print('Best test acc: {:4f}'.format(best_acc))\n",
    "    model.load_state_dict(best_model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-27T17:16:08.160126Z",
     "start_time": "2018-06-27T17:16:08.155128Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(model, dataloader):\n",
    "    cross_entropy = nn.CrossEntropyLoss(size_average=False)\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_corrects = 0\n",
    "    count = 0\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for packedseq, label in dataloader:\n",
    "            output = model(packedseq)\n",
    "            \n",
    "            loss = cross_entropy(output, label)\n",
    "            total_loss += loss.item()\n",
    "            count += label.shape[0]\n",
    "            correct = output.argmax(dim=1) == label\n",
    "            total_corrects += correct.item()\n",
    "            \n",
    "    loss = total_loss/count\n",
    "    accuracy = total_corrects/count\n",
    "    \n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-27T17:16:11.277290Z",
     "start_time": "2018-06-27T17:16:09.939019Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = WordDataset(category_lines, 1)\n",
    "test_data = WordDataset(category_lines, 0)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=1, shuffle=True, collate_fn=collate)\n",
    "test_dataloader = DataLoader(test_data, batch_size=1, shuffle=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-27T20:15:36.757126Z",
     "start_time": "2018-06-27T17:16:16.350587Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.2722 \tTest Loss: 1.0274 \tTest Accuracy: 0.7002\n",
      "Epoch: 2 \tTraining Loss: 0.9885 \tTest Loss: 0.9178 \tTest Accuracy: 0.7405\n",
      "Epoch: 3 \tTraining Loss: 0.9075 \tTest Loss: 0.8621 \tTest Accuracy: 0.7431\n",
      "Epoch: 4 \tTraining Loss: 0.8663 \tTest Loss: 0.8542 \tTest Accuracy: 0.7476\n",
      "Epoch: 5 \tTraining Loss: 0.8351 \tTest Loss: 0.8120 \tTest Accuracy: 0.7573\n",
      "Best val acc: 0.757273\n",
      "No. of layers: 1 \tNo. of hidden layers: 10 \tTest Loss: 0.8120 \tTest Accuracy: 0.7573\n",
      "Epoch: 1 \tTraining Loss: 1.2325 \tTest Loss: 0.9278 \tTest Accuracy: 0.7210\n",
      "Epoch: 2 \tTraining Loss: 0.9067 \tTest Loss: 0.8977 \tTest Accuracy: 0.7124\n",
      "Epoch: 3 \tTraining Loss: 0.7799 \tTest Loss: 0.7338 \tTest Accuracy: 0.7782\n",
      "Epoch: 4 \tTraining Loss: 0.6936 \tTest Loss: 0.6913 \tTest Accuracy: 0.7914\n",
      "Epoch: 5 \tTraining Loss: 0.6389 \tTest Loss: 0.6696 \tTest Accuracy: 0.7967\n",
      "Best val acc: 0.796675\n",
      "No. of layers: 1 \tNo. of hidden layers: 50 \tTest Loss: 0.6696 \tTest Accuracy: 0.7967\n",
      "Epoch: 1 \tTraining Loss: 1.2668 \tTest Loss: 1.0646 \tTest Accuracy: 0.6750\n",
      "Epoch: 2 \tTraining Loss: 0.9610 \tTest Loss: 0.8976 \tTest Accuracy: 0.7297\n",
      "Epoch: 3 \tTraining Loss: 0.8226 \tTest Loss: 0.7504 \tTest Accuracy: 0.7727\n",
      "Epoch: 4 \tTraining Loss: 0.7227 \tTest Loss: 0.8139 \tTest Accuracy: 0.7634\n",
      "Epoch: 5 \tTraining Loss: 0.6482 \tTest Loss: 0.6719 \tTest Accuracy: 0.7922\n",
      "Best val acc: 0.792186\n",
      "No. of layers: 1 \tNo. of hidden layers: 250 \tTest Loss: 0.6719 \tTest Accuracy: 0.7922\n",
      "Epoch: 1 \tTraining Loss: 1.4352 \tTest Loss: 1.1161 \tTest Accuracy: 0.6828\n",
      "Epoch: 2 \tTraining Loss: 1.0609 \tTest Loss: 1.0024 \tTest Accuracy: 0.7092\n",
      "Epoch: 3 \tTraining Loss: 0.9554 \tTest Loss: 0.8883 \tTest Accuracy: 0.7438\n",
      "Epoch: 4 \tTraining Loss: 0.8989 \tTest Loss: 0.9453 \tTest Accuracy: 0.7209\n",
      "Epoch: 5 \tTraining Loss: 0.8642 \tTest Loss: 0.8481 \tTest Accuracy: 0.7500\n",
      "Best val acc: 0.749958\n",
      "No. of layers: 2 \tNo. of hidden layers: 10 \tTest Loss: 0.8481 \tTest Accuracy: 0.7500\n",
      "Epoch: 1 \tTraining Loss: 1.4154 \tTest Loss: 1.1755 \tTest Accuracy: 0.6151\n",
      "Epoch: 2 \tTraining Loss: 1.0076 \tTest Loss: 0.9188 \tTest Accuracy: 0.7197\n",
      "Epoch: 3 \tTraining Loss: 0.8652 \tTest Loss: 0.7977 \tTest Accuracy: 0.7579\n",
      "Epoch: 4 \tTraining Loss: 0.7687 \tTest Loss: 0.7558 \tTest Accuracy: 0.7702\n",
      "Epoch: 5 \tTraining Loss: 0.6984 \tTest Loss: 0.7262 \tTest Accuracy: 0.7832\n",
      "Best val acc: 0.783209\n",
      "No. of layers: 2 \tNo. of hidden layers: 50 \tTest Loss: 0.7262 \tTest Accuracy: 0.7832\n",
      "Epoch: 1 \tTraining Loss: 1.4465 \tTest Loss: 1.2251 \tTest Accuracy: 0.6469\n",
      "Epoch: 2 \tTraining Loss: 1.0654 \tTest Loss: 0.9237 \tTest Accuracy: 0.7222\n",
      "Epoch: 3 \tTraining Loss: 0.9088 \tTest Loss: 0.8247 \tTest Accuracy: 0.7546\n",
      "Epoch: 4 \tTraining Loss: 0.7899 \tTest Loss: 0.7508 \tTest Accuracy: 0.7714\n",
      "Epoch: 5 \tTraining Loss: 0.7037 \tTest Loss: 0.6902 \tTest Accuracy: 0.7892\n",
      "Best val acc: 0.789194\n",
      "No. of layers: 2 \tNo. of hidden layers: 250 \tTest Loss: 0.6902 \tTest Accuracy: 0.7892\n"
     ]
    }
   ],
   "source": [
    "for num_layers in (1,2):\n",
    "    for num_hidden in (10, 50, 250):\n",
    "        model = LSTMmodel(input_size=n_letters, hidden_size=num_hidden, num_layers=num_layers, num_classes=n_categories)\n",
    "        train(model, train_dataloader=train_dataloader, test_dataloader=test_dataloader, train_data_len=len(train_data))\n",
    "        test_loss, test_acc = test(model, test_dataloader)\n",
    "        \n",
    "        print('No. of layers: {} \\tNo. of hidden layers: {} \\tTest Loss: {:.4f} \\tTest Accuracy: {:.4f}'.format(\n",
    "               num_layers, num_hidden, test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "459px",
    "left": "919px",
    "right": "20px",
    "top": "109px",
    "width": "533px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
